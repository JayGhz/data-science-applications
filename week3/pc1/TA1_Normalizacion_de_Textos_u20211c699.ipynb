{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmozbMh5CEAH"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "527lRI0zDtC2"
      },
      "source": [
        "<h3 align=\"center\"><b>TA1:</b> NLP - Normalización de textos </h3>\n",
        "<h3 align=\"center\">2025-2</h3>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfWr1IM-DMap"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Nombre del Alumno:**  \n",
        "**Código:**  \n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYdJ6qZIwWnQ"
      },
      "source": [
        "### **NLP: Normalización de textos y Bolsa de Palabras**\n",
        "\n",
        "* El corpus que se normalizará consiste en una serie de artículos obtenidos de la web \"https://www.elmundotoday.com/\".\n",
        "\n",
        "\n",
        "* Estos artículos se encuentran en el fichero csv **corpus_mundo_today.csv** que deberá adjuntar al notebook.\n",
        "\n",
        "\n",
        "* Este CSV esta formado por 3 campos que son:\n",
        "    - Tema\n",
        "    - Título\n",
        "    - Texto\n",
        "    \n",
        "    \n",
        "* El ejercicio consiste en Normalizar este ***Corpus*** tomando el *título* y *texto* como contenido de cada documento.\n",
        "\n",
        "Puede utilizar indistintamente las librerias **NLTK** y **Spacy** para el preprocesamiento (normalización) del texto.\n",
        "\n",
        "\n",
        "## Ejercicios de Nomalización solicitados:\n",
        "\n",
        "* Dada una lista en la que cada elemento de la misma tiene el contenido (título + texto) de cada documento del corpus se pide:\n",
        "<span></span><br><br>\n",
        "    1. **Crear una función que devuelva los documentos *Tokenizados* (una lista de listas) y con los tokens (palabras) en minúsculas.**\n",
        "        * **input**: lista de documentos (lista de Strings).\n",
        "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        "<span></span><br><br>\n",
        "    2. **Crear una función que elimine los tokens que sean signos de puntuación y *Stop-Words*.**\n",
        "        * **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        "<span></span><br><br>\n",
        "    3. **Crear una función que transforme cada token a su lema (*Lematización*)**\n",
        "        * **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        "<span></span><br><br>\n",
        "    4. **Crear una función que elimine todos los tokens que no sean *Nombres* (NOUN, PROPN), *Verbos*, *Advervios* o *Adjetivos*.**\n",
        "        * **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        " <span></span><br><br>       \n",
        "    5. **Función que dada una lista de documentos, devuelva los documentos normalizados. Este ejercicio ya esta hecho y simplemente tiene que funcionar llamando a las 4 funciones anteriores.**\n",
        "        * **input**: lista de documentos (lista de Strings).\n",
        "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento normalizados.\n",
        "\n",
        "\n",
        "* Finalizada la normalización anterior, se pide:\n",
        "\n",
        "    6. **Crear una función que dada una lista de documentos (*corpus*) tokenizados, elimine del corpus aquellos tokens que aparecen menos de 'N' veces (N=10) en el corpus**\n",
        "        * **input**: lista de listas, en la que cada lista contiene los tokens del documento normalizados.\n",
        "        * **input**: 'N' -> Parámetro que nos indica el número mínimo de apariciones de la palabra en el corpus.\n",
        "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento normalizados.\n",
        "<span></span><br><br>\n",
        "   \n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sv6qkBgRwWnX"
      },
      "source": [
        "## Ejercicios de Nomalización:\n",
        "\n",
        "* Leemos el corpus y pasamos los documentos (Título + Texto) a una lista"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "MtvRq_btwWnY"
      },
      "outputs": [],
      "source": [
        "docs_file = 'corpus_mundo_today.csv'\n",
        "docs_list = list()\n",
        "file_txt = open(docs_file, encoding=\"utf8\").read()\n",
        "for line in file_txt.split('\\n'):\n",
        "    line = line.split('||')\n",
        "    docs_list.append(line[1] + ' ' + line[2])\n",
        "docs_list = docs_list[1:] # Elimino la cabecera del fichero"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7A1rAYSwWna"
      },
      "source": [
        "#### 1. **Crear una función que devuelva los documentos *Tokenizados* (una lista de listas) y con los tokens (palabras) en minúsculas.** (3ptos)\n",
        "\n",
        "* **input**: lista de documentos (lista de Strings).\n",
        "* **output**: lista de listas, en la que cada lista contiene los tokens del documento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "T-4TOCb_wWnb"
      },
      "outputs": [],
      "source": [
        "def tokenization(docs_list):\n",
        "    tokenized_docs = []\n",
        "\n",
        "    for doc in docs_list:\n",
        "        # tokenizacion y conversion a minisculas\n",
        "        tokens = doc.lower().split()\n",
        "        tokenized_docs.append(tokens)\n",
        "\n",
        "    return tokenized_docs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJTRkdaNwWnb"
      },
      "source": [
        "#### 2. **Crear una función que elimine los tokens que sean signos de puntuación y *Stop-Words*.** (3ptos)\n",
        "\n",
        "* **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        "* **output**: lista de listas, en la que cada lista contiene los tokens del documento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgGvvCzcE0sW",
        "outputId": "84f2267a-85bf-40c6-e606-f390a9a0fe5e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "u_Zij4h5wWnc"
      },
      "outputs": [],
      "source": [
        "def remove_words(docs):\n",
        "    stop_words = set(stopwords.words('spanish'))\n",
        "\n",
        "    punctuation = set([\n",
        "    '.', ',', ':', ';', '!', '?', '(', ')',\n",
        "    '[', ']', '\"', \"'\", '-', '--', '...'\n",
        "    ])\n",
        "\n",
        "    cleaned_docs = []\n",
        "    for doc in docs:\n",
        "        cleaned_tokens = [token for token in doc if token.lower() not in stop_words and token not in punctuation]\n",
        "        cleaned_docs.append(cleaned_tokens)\n",
        "\n",
        "    return cleaned_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVZeM62QwWnd"
      },
      "source": [
        "#### 3. **Crear una función que transforme cada token a su lema (*Lematización*)** (3ptos)\n",
        "\n",
        "* **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        "* **output**: lista de listas, en la que cada lista contiene los tokens del documento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEtGUMxUwWnd",
        "outputId": "29fcd3ba-75a8-4c0d-eef9-de6eeda77b1e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def lematization(docs):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_docs = []\n",
        "    for doc in docs:\n",
        "        lemmas = [lemmatizer.lemmatize(token) for token in doc]\n",
        "        lemmatized_docs.append(lemmas)\n",
        "    return lemmatized_docs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdz8mIcCwWne"
      },
      "source": [
        "#### 4. **Crear una función que elimine todos los tokens que no sean *Nombres* (NOUN, PROPN), *Verbos*, *Advervios* o *Adjetivos*.** (4ptos)\n",
        "\n",
        "* **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        "* **output**: lista de listas, en la que cada lista contiene los tokens del documento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1Mj1fckE0sZ",
        "outputId": "7285b81c-c47b-44d3-b658-832d5e7dd6f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'es' are deprecated. Please use the\n",
            "full pipeline package name 'es_core_news_sm' instead.\u001b[0m\n",
            "Collecting es-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "! python -m spacy download es\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "K4v5aGv5wWne"
      },
      "outputs": [],
      "source": [
        "\n",
        "import spacy\n",
        "nlp = spacy.load('es_core_news_sm')\n",
        "\n",
        "def filter_words(docs):\n",
        "    filtered_docs = []\n",
        "    for doc in docs:\n",
        "        text = ' '.join(doc)\n",
        "        doc_spacy = nlp(text)\n",
        "\n",
        "        filtered_tokens = [token.text for token in doc_spacy if token.pos_ in ['NOUN', 'PROPN', 'VERB', 'ADV', 'ADJ']]\n",
        "        filtered_docs.append(filtered_tokens)\n",
        "\n",
        "    return filtered_docs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0h29B0IwWne"
      },
      "source": [
        "#### 5. **Función que dada una lista de documentos, devuelva los documentos normalizados. Este ejercicio ya esta hecho y simplemente tiene que funcionar llamando a las 4 funciones anteriores.** (3ptos)\n",
        "\n",
        "* **input**: lista de documentos (lista de Strings).\n",
        "* **output**: lista de listas, en la que cada lista contiene los tokens del documento normalizados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rzYM9jRwWnf",
        "outputId": "597e5742-86db-4b90-cca9-925bcaaf52b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['gobierno', 'español', 'sumará', 'junqueras', 'condenas', 'cumplir', 'puigdemont', 'después', 'revés', 'recibido', 'gobierno', 'españa', 'puesta', 'libertad', 'carles', 'puigdemont', 'parte', 'justicia', 'alemana', 'juez', 'pablo', 'llarena', 'decidido', 'semana', 'instancias', 'ejecutivo', 'sumará', 'oriol', 'junqueras', 'condenas', 'cumplir', 'líder', 'pdecat', 'exvicepresidente', 'cataluña', 'permanece', 'prisión', 'madrileña', 'estremera', 'pasado', 'noviembre', 'asumiría', 'delitos', 'atribuidos', 'carles', 'puigdemont', 'manera', 'tribunal', 'supremo', 'asegura', 'actos', 'expresidente', 'catalán', 'última', 'legislatura', 'quedan', 'impunes', '“', 'junqueras', 'pagará', 'maniobra', 'ideada', 'burlar', 'justicia', 'alemana', 'líder', 'esquerra', 'republicana', 'enfrenta', 'años', 'prisión', 'seguiremos', 'adelante', 'junqueras', 'caigan', 'años', 'más', 'parar', 'dicho', 'hoy', 'carles', 'puigdemont', 'alemania', 'haré', 'hacer', 'junqueras', 'sacrificar', 'asumiré', 'resignación', 'determinación', '”', 'prometido', '“', 'seguim', 'tuiteaba', 'después', 'trascender', 'decisión', 'llarena', 'fuentes', 'anónimas', 'poder', 'judicial', 'barajando', 'posibilidad', 'añadir', 'pena', 'oriol', 'junqueras', 'condenas', 'imponerse', 'futuro', 'iñaki', 'urdangarin', 'rodrigo', 'rato', 'esperanza', 'aguirre', 'así', 'delito', 'robo', 'fuerza', 'ocurrido', 'hace', 'semana', 'huesca', 'policía', 'incapaz', 'encontrar', 'culpable']\n"
          ]
        }
      ],
      "source": [
        "def normalization(docs_list):\n",
        "    corpus = tokenization(docs_list)\n",
        "    corpus = remove_words(corpus)\n",
        "    corpus = lematization(corpus)\n",
        "    corpus = filter_words(corpus)\n",
        "    return corpus\n",
        "\n",
        "corpus = normalization(docs_list)\n",
        "print(corpus[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqLNb64MO9G0"
      },
      "source": [
        "#### En este ejercicio podemos ver como reducimos las palabras (tokens) del texto original, quedandonos con lo importante y normalizado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "CvY3GxDQJMha"
      },
      "outputs": [],
      "source": [
        "def get_tokens(text):\n",
        "    return text.split()\n",
        "\n",
        "def normalize(text):\n",
        "    return filter_words(lematization(remove_words(tokenization([text]))))[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfHzz4xvJUUq",
        "outputId": "92de305f-6852-47fb-d330-6d1a1ab998e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El Gobierno español sumará a Junqueras las condenas que no vaya a cumplir Puigdemont Después del revés recibido por el Gobierno de España tras la puesta en libertad de Carles Puigdemont por parte de la justicia alemana, el juez Pablo Llarena ha decidido esta semana, a instancias del Ejecutivo, que sumará a Oriol Junqueras las condenas que no vaya a cumplir el líder del PDeCAT. El exvicepresidente de Cataluña, que permanece en la prisión madrileña de Estremera desde el pasado dos de noviembre, asumiría por tanto todos los delitos atribuidos a Carles Puigdemont y, de esta manera, el Tribunal Supremo se asegura de que los actos del expresidente catalán durante la última legislatura no quedan impunes, ya que “Junqueras pagará por todos y cada uno de ellos”. Con esta maniobra ideada para burlar la justicia alemana, el líder de Esquerra Republicana se enfrenta a 50 años más de prisión. “Seguiremos adelante aunque a Junqueras le caigan cien años más, nadie nos va a parar”, ha dicho hoy Carles Puigdemont desde Alemania. “Haré lo que tenga que hacer y si Junqueras se tiene que sacrificar por ello, lo asumiré con resignación y determinación”, ha prometido. “Seguim!”, tuiteaba poco después de trascender la decisión de Llarena. Según fuentes anónimas del poder judicial, se está barajando también la posibilidad de añadir a la pena de Oriol Junqueras las condenas que puedan imponerse en el futuro a Iñaki Urdangarin, Rodrigo Rato o Esperanza Aguirre, entre otros, así como la de un delito de robo con fuerza ocurrido hace una semana en Huesca y del que la policía ha sido incapaz de encontrar al culpable.\n"
          ]
        }
      ],
      "source": [
        "raw = docs_list[0]\n",
        "print(raw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAN8QsYSGZ-r",
        "outputId": "250f0fcf-a1d2-4b81-b171-b5a36dbb3d77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Número de tokens del texto original: 270\n",
            "Número de tokens distintos del texto original: 165\n",
            "Número de tokens tras la normalización: 130\n",
            "Número de tokens distintos tras la normalización: 105\n"
          ]
        }
      ],
      "source": [
        "print('Número de tokens del texto original: ' + str(len(get_tokens(raw))))\n",
        "print('Número de tokens distintos del texto original: ' + str(len(set(get_tokens(raw)))))\n",
        "print('Número de tokens tras la normalización: ' + str(len(normalize(raw))))\n",
        "print('Número de tokens distintos tras la normalización: ' + str(len(set(normalize(raw)))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6NzHFCMwWnf"
      },
      "source": [
        "<hr>\n",
        "\n",
        "#### 6. **Crear una función que dada una lista de documentos (*corpus*) tokenizados, elimine del corpus aquellos tokens que aparecen menos de 'N' veces (N=10) en el corpus** (4ptos)\n",
        "\n",
        "* **input**: lista de listas, en la que cada lista contiene los tokens del documento normalizados.\n",
        "* **input**: 'N' -> Parámetro que nos indica el número mínimo de apariciones de la palabra en el corpus.\n",
        "* **output**: lista de listas, en la que cada lista contiene los tokens del documento normalizados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogj3An3ywWnf",
        "outputId": "d9336b49-a4ca-4372-e18d-54b1b94aec59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['gobierno', 'puigdemont', 'después', 'gobierno', 'españa', 'puigdemont', 'parte', 'semana', 'líder', 'cataluña', 'pasado', 'puigdemont', 'catalán', 'última', '“', 'líder', 'años', 'años', 'más', 'dicho', 'hoy', 'puigdemont', 'hacer', '”', '“', 'después', 'así', 'hace', 'semana']\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def drop_less_frecuency_words(corpus, n):\n",
        "    all_tokens = [token for doc in corpus for token in doc]\n",
        "    token_counts = Counter(all_tokens)\n",
        "\n",
        "    filtered_corpus = []\n",
        "    for doc in corpus:\n",
        "        filtered_doc = [token for token in doc if token_counts[token] >= n]\n",
        "        filtered_corpus.append(filtered_doc)\n",
        "\n",
        "    return filtered_corpus\n",
        "\n",
        "corpus = drop_less_frecuency_words(corpus, 10)\n",
        "print(corpus[0])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
