{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXjOv0sB_N8b"
      },
      "source": [
        "### Uso de NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SACapOdX9sQ8",
        "outputId": "e895f38a-5a55-4ae0-b1ca-2a8aa64dd3d9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package cess_esp to\n",
            "[nltk_data]     C:\\Users\\jayka\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package cess_esp is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "#descargar el corpus en español\n",
        "nltk.download(\"cess_esp\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTIBg5zG_UeZ"
      },
      "source": [
        "### Expresiones regulares"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85rav7DS9-z9",
        "outputId": "6cfd2846-2e8e-42e6-8bc8-12a0f0fc924c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['El', 'grupo', 'estatal', 'Electricité_de_France', '-Fpa-', 'EDF', '-Fpt-', 'anunció', 'hoy', ',', 'jueves', ',', 'la', 'compra', 'del', '51_por_ciento', 'de', 'la', 'empresa', 'mexicana', 'Electricidad_Águila_de_Altamira', '-Fpa-', 'EAA', '-Fpt-', ',', 'creada', 'por', 'el', 'japonés', 'Mitsubishi_Corporation', 'para', 'poner_en_marcha', 'una', 'central', 'de', 'gas', 'de', '495', 'megavatios', '.'], ['Una', 'portavoz', 'de', 'EDF', 'explicó', 'a', 'EFE', 'que', 'el', 'proyecto', 'para', 'la', 'construcción', 'de', 'Altamira_2', ',', 'al', 'norte', 'de', 'Tampico', ',', 'prevé', 'la', 'utilización', 'de', 'gas', 'natural', 'como', 'combustible', 'principal', 'en', 'una', 'central', 'de', 'ciclo', 'combinado', 'que', 'debe', 'empezar', 'a', 'funcionar', 'en', 'mayo_del_2002', '.'], ...]\n",
            "6030\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "corpus = nltk.corpus.cess_esp.sents()\n",
        "\n",
        "print(corpus)\n",
        "print(len(corpus))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "O0bP1_OCAs6x"
      },
      "outputs": [],
      "source": [
        "# Aplanar una lista compuesta de sublistas\n",
        "\n",
        "flatten = [w for l in corpus for w in l]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_vp-Wq7BhVq",
        "outputId": "e2e1f227-4f46-410b-b403-679802d85468"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "192686"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(flatten)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98m5FjA-Bq0d",
        "outputId": "dd5b75bb-8916-4d3c-e1b4-4c305614f79c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['El', 'grupo', 'estatal', 'Electricité_de_France', '-Fpa-', 'EDF', '-Fpt-', 'anunció', 'hoy', ',', 'jueves', ',', 'la', 'compra', 'del', '51_por_ciento', 'de', 'la', 'empresa', 'mexicana']\n"
          ]
        }
      ],
      "source": [
        "print(flatten[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2rghBc8CFNK"
      },
      "source": [
        "Aplicacion de expresiones regulares sobre la variable **flatten**:\n",
        "\n",
        "re.search(p,s)\n",
        "\n",
        "p = patron de busqueda\n",
        "s = cadena donde buscar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIjtvgOFCMX2",
        "outputId": "ed8f9abc-9686-44b4-b883-b2cc4d1038f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['estatal', 'jueves', 'empresa', 'centrales', 'francesa']"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Meta-caracteres basicos\n",
        "\n",
        "arr = [w for w in flatten if re.search(\"es\", w)]\n",
        "\n",
        "arr[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rguHBFA1DcnO",
        "outputId": "ef4251e7-aa7b-4842-f0c0-10d9e6cf361b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['jueves', 'centrales', 'millones', 'millones', 'dólares']"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "arr = [w for w in flatten if re.search(\"es$\", w)]\n",
        "\n",
        "arr[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lf0timi3DqWt",
        "outputId": "357dd587-e314-4990-e056-8b7c279b4081"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['tajantes']"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "arr = [w for w in flatten if re.search(\"^..j..t..$\", w)]\n",
        "\n",
        "arr[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEV2X6vLEUxR",
        "outputId": "dea4202b-4f18-4e1e-b02c-97a3326e7ad7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['golf', 'golf']"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Aplicacion de rangos\n",
        "# [a-z], [A-Z], [0-9]\n",
        "arr = [w for w in flatten if re.search(\"^[ghi][mno][jlk][def]$\", w)]\n",
        "\n",
        "arr[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSAxM7TN88XV"
      },
      "source": [
        "## Normalizacion del Texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiR8tDZ-92Nf"
      },
      "source": [
        "(1) Tokenizacion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7zDPPIS-VCe",
        "outputId": "886762e5-109c-40b7-c274-7cf6b2412712"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cuando sea presidente del mundo (imaginaba en mi cabeza) no tendre que preocuparme por tantas tonteras.\n",
            "Era solo un niño de 10 años, pero pensaba que podria ser cualquier cosa en su imaginacion...\n"
          ]
        }
      ],
      "source": [
        "texto = \"\"\"Cuando sea presidente del mundo (imaginaba en mi cabeza) no tendre que preocuparme por tantas tonteras.\n",
        "Era solo un niño de 10 años, pero pensaba que podria ser cualquier cosa en su imaginacion...\"\"\"\n",
        "\n",
        "print(texto)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42pz-deq_RSd",
        "outputId": "b2a24cb0-9280-4c10-98e6-f891c67bc76d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Cuando', 'sea', 'presidente', 'del', 'mundo', '(imaginaba', 'en', 'mi', 'cabeza)', 'no', 'tendre', 'que', 'preocuparme', 'por', 'tantas', 'tonteras.\\nEra', 'solo', 'un', 'niño', 'de', '10', 'años,', 'pero', 'pensaba', 'que', 'podria', 'ser', 'cualquier', 'cosa', 'en', 'su', 'imaginacion...']\n"
          ]
        }
      ],
      "source": [
        "#Caso 1\n",
        "print(re.split(r' ', texto))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXCUO-_J_8Fo",
        "outputId": "f4e9cfe9-2a49-4b44-d201-2871862f1f06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Cuando', 'sea', 'presidente', 'del', 'mundo', '(imaginaba', 'en', 'mi', 'cabeza)', 'no', 'tendre', 'que', 'preocuparme', 'por', 'tantas', 'tonteras.', 'Era', 'solo', 'un', 'niño', 'de', '10', 'años,', 'pero', 'pensaba', 'que', 'podria', 'ser', 'cualquier', 'cosa', 'en', 'su', 'imaginacion...']\n"
          ]
        }
      ],
      "source": [
        "#Caso 2:\n",
        "print(re.split(r'[ \\t\\n]+', texto))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rd1GqRxAaVp",
        "outputId": "a51f8b08-b123-4b23-a3bd-9bced5071f6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Cuando', 'sea', 'presidente', 'del', 'mundo', 'imaginaba', 'en', 'mi', 'cabeza', 'no', 'tendre', 'que', 'preocuparme', 'por', 'tantas', 'tonteras', 'Era', 'solo', 'un', 'niño', 'de', '10', 'años', 'pero', 'pensaba', 'que', 'podria', 'ser', 'cualquier', 'cosa', 'en', 'su', 'imaginacion', '']\n"
          ]
        }
      ],
      "source": [
        "#Caso 3:\n",
        "# \\W omite todos los caracteres que nos sean letras, digitos o guiones\n",
        "print(re.split(r'[ \\W\\t\\n]+', texto))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JRxhZQ_9ARl",
        "outputId": "2f07f566-6f35-4428-8bf8-19ab01f5216b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['En', 'los', 'E', 'U', 'esa', 'hamburguesa', 'cuesta', '15', '50']\n"
          ]
        }
      ],
      "source": [
        "texto = \"En los E.U. esa hamburguesa cuesta $15.50\"\n",
        "print(re.split(r'[ \\W\\t\\n]+',texto))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "JMFBeGqhBi5f"
      },
      "outputs": [],
      "source": [
        "pattern = r'''(?x)                 # set flag to allow verbose regexps\n",
        "              (?:[A-Z]\\.)+         # abbreviations, e.g. U.S.A.\n",
        "              | \\w+(?:-\\w+)*       # words with optional internal hyphens\n",
        "              | \\$?\\d+(?:\\.\\d+)?%? # currency and percentages, e.g. $12.40, 82%\n",
        "              | \\.\\.\\.             # ellipsis\n",
        "              | [][.,;\"'?():-_`]   # these are separate tokens; includes ], [\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLYphGDNDCdS",
        "outputId": "a19c363e-8e6a-454a-94f9-c06384f7a03e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['En', 'los', 'E.U.', 'esa', 'hamburguesa', 'cuesta', '$15.50']"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.regexp_tokenize(texto, pattern)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqZ7vvrAEIAk"
      },
      "source": [
        "(2) Lematizacion (encontrar la raiz linguistica de una palabra)\n",
        "\n",
        "Stemming = lematizacion simple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-WJG9u5EPlV",
        "outputId": "205d0d90-b66a-4dfa-f4e3-9603ae9f4323"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('arabic',\n",
              " 'danish',\n",
              " 'dutch',\n",
              " 'english',\n",
              " 'finnish',\n",
              " 'french',\n",
              " 'german',\n",
              " 'hungarian',\n",
              " 'italian',\n",
              " 'norwegian',\n",
              " 'porter',\n",
              " 'portuguese',\n",
              " 'romanian',\n",
              " 'russian',\n",
              " 'spanish',\n",
              " 'swedish')"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "SnowballStemmer.languages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "8NBaDQrFFHrp"
      },
      "outputs": [],
      "source": [
        "stem = SnowballStemmer('spanish')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZYBKglCEFQC3",
        "outputId": "8b59b6f3-5736-47c4-f222-717c86c6e9ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'trabaj'"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stem.stem(\"trabajaremos\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "oFLQxiP1Fqmf"
      },
      "outputs": [],
      "source": [
        "#Lematizacion\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemm= WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0PQVEVvF8Cl",
        "outputId": "268a5059-130b-479b-dd56-90c629ccaf3c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\jayka\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download(\"wordnet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7jz9kHSgGK12",
        "outputId": "8d2c1c13-30f5-4e48-d720-ea1f6cb20972"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'trabajó'"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lemm.lemmatize('trabajó')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjoF-eNiGn1d"
      },
      "source": [
        "## Uso de Spacy en NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrU39xqKTR-R"
      },
      "source": [
        "**spacy** es una librería de procesamiento del lenguaje natural, robusta, rápida, fácil de instalar y utilizar e integrable con otras librerías de NLP y de deep learning.\n",
        "\n",
        "Tiene modelos entrenados en varios idiomas y permite realizar las típicas tareas de segmentación por oraciones, tokenizanción, análisis morfológico, extracción de entidades y análisis de opinión.\n",
        "\n",
        "Una vez instalados los modelos, podemos importarlos fácilmente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKYCTJ_8GyIs",
        "outputId": "44b011a8-3178-4011-c7be-b5f2fd95b921"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in c:\\users\\jayka\\onedrive\\documentos\\courses\\data-science-applications\\env\\lib\\site-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\jayka\\onedrive\\documentos\\courses\\data-science-applications\\env\\lib\\site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\jayka\\onedrive\\documentos\\courses\\data-science-applications\\env\\lib\\site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\jayka\\onedrive\\documentos\\courses\\data-science-applications\\env\\lib\\site-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\jayka\\onedrive\\documentos\\courses\\data-science-applications\\env\\lib\\site-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\jayka\\onedrive\\documentos\\courses\\data-science-applications\\env\\lib\\site-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\jayka\\onedrive\\documentos\\courses\\data-science-applications\\env\\lib\\site-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\jayka\\onedrive\\documentos\\courses\\data-science-applications\\env\\lib\\site-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\jayka\\onedrive\\documentos\\courses\\data-science-applications\\env\\lib\\site-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\jayka\\onedrive\\documentos\\courses\\data-science-applications\\env\\lib\\site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\jayka\\onedrive\\documentos\\courses\\data-science-applications\\env\\lib\\site-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\jayka\\onedrive\\documentos\\courses\\data-science-applications\\env\\lib\\site-packages (from spacy) (0.17.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\jayka\\onedrive\\documentos\\courses\\data-science-applications\\env\\lib\\site-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\jayka\\onedrive\\documentos\\courses\\data-science-applications\\env\\lib\\site-packages (from spacy) (2.3.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\jayka\\onedrive\\documentos\\courses\\data-science-applications\\env\\lib\\site-packages (from spacy) (2.32.5)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\jayka\\onedrive\\documentos\\courses\\data-science-applications\\env\\lib\\site-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\jayka\\onedrive\\documentos\\courses\\data-science-applications\\env\\lib\\site-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in c:\\users\\jayka\\onedrive\\documentos\\courses\\data-science-applications\\env\\lib\\site-packages (from spacy) (80.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\jayka\\onedrive\\documentos\\courses\\data-science-applications\\env\\lib\\site-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\jayka\\onedrive\\documentos\\courses\\data-science-applications\\env\\lib\\site-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in c:\\users\\jayka\\onedrive\\documentos\\courses\\data-science-applications\\env\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jayka\\onedrive\\documentos\\courses\\data-science-applications\\env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\jayka\\onedrive\\documentos\\courses\\data-science-applications\\env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\jayka\\onedrive\\documentos\\courses\\data-science-applications\\env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\jayka\\onedrive\\documentos\\courses\\data-science-applications\\env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\jayka\\onedrive\\documentos\\courses\\data-science-applications\\env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jayka\\onedrive\\documentos\\courses\\data-science-applications\\env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jayka\\onedrive\\documentos\\courses\\data-science-applications\\env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jayka\\onedrive\\documentos\\courses\\data-science-applications\\env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\jayka\\onedrive\\documentos\\courses\\data-science-applications\\env\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\jayka\\onedrive\\documentos\\courses\\data-science-applications\\env\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: colorama in c:\\users\\jayka\\onedrive\\documentos\\courses\\data-science-applications\\env\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
            "Requirement already satisfied: click>=8.0.0 in c:\\users\\jayka\\onedrive\\documentos\\courses\\data-science-applications\\env\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\jayka\\onedrive\\documentos\\courses\\data-science-applications\\env\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in c:\\users\\jayka\\onedrive\\documentos\\courses\\data-science-applications\\env\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.1.0)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\jayka\\onedrive\\documentos\\courses\\data-science-applications\\env\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.22.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\jayka\\onedrive\\documentos\\courses\\data-science-applications\\env\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in c:\\users\\jayka\\onedrive\\documentos\\courses\\data-science-applications\\env\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\jayka\\onedrive\\documentos\\courses\\data-science-applications\\env\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\jayka\\onedrive\\documentos\\courses\\data-science-applications\\env\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\jayka\\onedrive\\documentos\\courses\\data-science-applications\\env\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\jayka\\onedrive\\documentos\\courses\\data-science-applications\\env\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jayka\\onedrive\\documentos\\courses\\data-science-applications\\env\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "! pip install -U spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVIJYU9eG7s0",
        "outputId": "2cd03755-bd48-4d15-a638-d66b260cf00e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'es' are deprecated. Please use the\n",
            "full pipeline package name 'es_core_news_sm' instead.\u001b[0m\n",
            "Collecting es-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
            "     ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
            "     --------------------------------------- 12.9/12.9 MB 64.9 MB/s eta 0:00:00\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "#Descargamos el modelo de Spacy en español\n",
        "! python -m spacy download es"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "OFe2jbIeHSos"
      },
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "gaCumO75HZYz"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('es_core_news_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "YPUUszhzIneC"
      },
      "outputs": [],
      "source": [
        "text = \"Soy un texto.  Normalmente soy más largo y más grande.  Que no te engañe mi tamaño mayor a 10 palabras\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "1nvb-jGgI7BM"
      },
      "outputs": [],
      "source": [
        "doc = nlp(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-UerA9zJA-s",
        "outputId": "500c56de-e31c-4a8f-d90d-90cde4ce60f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Soy',\n",
              " 'un',\n",
              " 'texto',\n",
              " '.',\n",
              " ' ',\n",
              " 'Normalmente',\n",
              " 'soy',\n",
              " 'más',\n",
              " 'largo',\n",
              " 'y',\n",
              " 'más',\n",
              " 'grande',\n",
              " '.',\n",
              " ' ',\n",
              " 'Que',\n",
              " 'no',\n",
              " 'te',\n",
              " 'engañe',\n",
              " 'mi',\n",
              " 'tamaño',\n",
              " 'mayor',\n",
              " 'a',\n",
              " '10',\n",
              " 'palabras']"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Creamos una lista con los tokens del texto\n",
        "tokens = [t.orth_ for t in doc]\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZPwEl6iJyar"
      },
      "source": [
        "**Limpiando el texto en Spacy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxdPFIOXKgrN"
      },
      "source": [
        "Omitir:\n",
        "\n",
        "- Palabras comunes (y, o, ni, que)\n",
        "- Preposiciones (a, en, para, por, entre, otras)\n",
        "- Verbos (ser)\n",
        "- Las puntuaciones\n",
        "\n",
        "Todas aquellas palabras que no aportan un significado importante en el texto las denominaremos stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dmr2Sd5sKutb",
        "outputId": "be95d3f8-d992-4bc7-8715-bb2b9274f374"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['texto', 'Normalmente', 'engañe', 'tamaño', '10', 'palabras']"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens_validos = [t.orth_ for t in doc if (not t.is_punct | t.is_stop) and t.orth_ != ' ']\n",
        "tokens_validos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXidc8udKmrk"
      },
      "source": [
        "**Normalizar texto en Spacy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLra4LqmJ2pr",
        "outputId": "40070884-ed92-47bf-b90d-3e8b6a74d68a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['texto', 'normalmente', 'engañe', 'tamaño', 'palabras']"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "words = [t.lower() for t in tokens_validos if len(t)>3 and t.isalpha()]\n",
        "words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xr7CNuUcXlm-"
      },
      "source": [
        "## **Similitud semántica entre palabras, frases y documentos**\n",
        "\n",
        "spaCy permite calcular la similitud semántica entre cualquier par de objetos de tipo Doc, Span o Token.\n",
        "\n",
        "Ojo, La similitud semántica es un concepto algo subjetivo, pero en este caso se puede entender como la probabilidad de que dos palabras aparezcan en los mismos contextos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "a90n_8y3W_bO"
      },
      "outputs": [],
      "source": [
        "from spacy import displacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
            "     --------------------------------- ----- 11.0/12.8 MB 53.4 MB/s eta 0:00:01\n",
            "     --------------------------------------- 12.8/12.8 MB 50.1 MB/s eta 0:00:00\n",
            "Installing collected packages: en-core-web-sm\n",
            "Successfully installed en-core-web-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "! python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "fCwXNGTnXAzP"
      },
      "outputs": [],
      "source": [
        "nlp_en = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUjA16sFYPWk",
        "outputId": "049669ca-4dc9-4548-c9a4-961cba66632c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cats vs dogs 0.3506028354167938\n",
            "research vs development 0.25495645403862\n",
            "cats vs development 0.050141848623752594\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\jayka\\AppData\\Local\\Temp\\ipykernel_6924\\1568170380.py:6: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  print(token1, \"vs\", token2, token1.similarity(token2))\n",
            "C:\\Users\\jayka\\AppData\\Local\\Temp\\ipykernel_6924\\1568170380.py:7: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  print(token3, \"vs\", token4, token3.similarity(token4))\n",
            "C:\\Users\\jayka\\AppData\\Local\\Temp\\ipykernel_6924\\1568170380.py:8: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  print(token1, \"vs\", token4, token1.similarity(token4))\n"
          ]
        }
      ],
      "source": [
        "# analizamos algunas colocaciones en inglés\n",
        "token1, _, token2 = nlp_en(\"cats and dogs\")\n",
        "token3, _, token4 = nlp_en(\"research and development\")\n",
        "\n",
        "# medimos la similitud semántica entre algunos pares\n",
        "print(token1, \"vs\", token2, token1.similarity(token2))\n",
        "print(token3, \"vs\", token4, token3.similarity(token4))\n",
        "print(token1, \"vs\", token4, token1.similarity(token4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Yac1-xYYPkT",
        "outputId": "eedb41ab-8a65-41a8-bd71-ba02f16147cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "perros vs gatos 0.6791152358055115\n",
            "investigación vs desarrollo 0.2735188603401184\n",
            "perros vs desarrollo 0.14307770133018494\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\jayka\\AppData\\Local\\Temp\\ipykernel_6924\\314909809.py:6: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  print(token1, \"vs\", token2, token1.similarity(token2))\n",
            "C:\\Users\\jayka\\AppData\\Local\\Temp\\ipykernel_6924\\314909809.py:7: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  print(token3, \"vs\", token4, token3.similarity(token4))\n",
            "C:\\Users\\jayka\\AppData\\Local\\Temp\\ipykernel_6924\\314909809.py:8: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  print(token1, \"vs\", token4, token1.similarity(token4))\n"
          ]
        }
      ],
      "source": [
        "# ¿qué tal funciona en español?\n",
        "token1, _, token2 = nlp(\"perros y gatos\")\n",
        "token3, _, token4 = nlp(\"investigación y desarrollo\")\n",
        "\n",
        "# medimos la similitud semántica entre algunos pares\n",
        "print(token1, \"vs\", token2, token1.similarity(token2))\n",
        "print(token3, \"vs\", token4, token3.similarity(token4))\n",
        "print(token1, \"vs\", token4, token1.similarity(token4))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
